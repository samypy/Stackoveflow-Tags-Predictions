import spacy
from spacy.training.example import Example

# Load a blank English model
nlp = spacy.blank("en")

# Add a new entity label for custom person names
nlp.add_pipe("ner", config={"labels": ["CUSTOM_PERSON"]})

# Define your training data
# Example data format: (text, {"entities": [(start, end, "CUSTOM_PERSON")]})
training_data = [
    ("Jan did a great job but Sam needs improvement.", {"entities": [(0, 3, "CUSTOM_PERSON"), (28, 31, "CUSTOM_PERSON")]}),
    # Add more examples as needed
]

# Convert training data to spaCy format
examples = []
for text, annotations in training_data:
    example = Example.from_dict(nlp.make_doc(text), annotations)
    examples.append(example)

# Train the NER model
nlp.begin_training()
for epoch in range(10):  # You might need to adjust the number of epochs
    for example in examples:
        nlp.update([example], drop=0.5)  # You can adjust the dropout rate

# Save the trained model
nlp.to_disk("path/to/your/custom_ner_model")

import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming your DataFrame is named 'df' with columns 'Name', 'Sentence', and 'Index'
# Example data:
# df = pd.DataFrame({'Name': ['John', 'Alice', 'Bob', 'Charlie'],
#                    'Sentence': ['John is a programmer.', 'Alice likes coding.', 'Bob enjoys hiking.', 'Charlie and David are friends.'],
#                    'Index': [(0, 3), (0, 5), (0, 2), (0, 7)]})

# Load a blank English model
nlp = spacy.blank("en")

# Add a new entity label for custom person names
nlp.add_pipe("ner", config={"labels": ["CUSTOM_PERSON"]})

# Convert DataFrame to spaCy training data format
training_data = []

for _, row in df.iterrows():
    name = row['Name']
    sentence = row['Sentence']
    start, end = row['Index']

    # Create a training example in spaCy format
    example = Example.from_dict(nlp.make_doc(sentence), {"entities": [(start, end, "CUSTOM_PERSON")]})
    training_data.append(example)

# Train the NER model
nlp.begin_training()
for epoch in range(10):  # You might need to adjust the number of epochs
    for example in training_data:
        nlp.update([example], drop=0.5)  # You can adjust the dropout rate

# Save the trained model
nlp.to_disk("path/to/your/custom_ner_name_model")

import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming your DataFrame is named 'df' with columns 'Name', 'Sentence', and 'Index'
# Example data:
# df = pd.DataFrame({'Name': ['John', 'Alice', 'Bob', 'Charlie'],
#                    'Sentence': ['John is a programmer.', 'Alice likes coding.', 'Bob enjoys hiking.', 'Charlie and David are friends.'],
#                    'Index': [(0, 3), (0, 5), (0, 2), (0, 7)]})

# Load a blank English model
nlp = spacy.blank("en")

# Create a new entity label for custom person names
ner = nlp.add_pipe("ner")

# Convert DataFrame to spaCy training data format
training_data = []

for _, row in df.iterrows():
    name, sentence, index = row['Name'], row['Sentence'], row['Index']
    start, end = index

    # Create a training example in spaCy format
    example = Example.from_dict(nlp.make_doc(sentence), {"entities": [(start, end, "CUSTOM_PERSON")]})
    training_data.append(example)

# Define a callback to print training progress
def print_progress(info, epoch, loss, scorer):
    print(f"Epoch: {epoch}, Loss: {loss}, Progress: {info['progress']}")

# Train the NER model with the callback
nlp.begin_training(callback=print_progress)
for epoch in range(10):  # You might need to adjust the number of epochs
    for example in training_data:
        nlp.update([example], drop=0.5)  # You can adjust the dropout rate

# Save the trained model
nlp.to_disk("path/to/your/custom_ner_name_model")

import pandas as pd
from reportlab.pdfgen import canvas

def excel_to_pdf(input_excel, output_pdf):
    # Read all sheets from Excel file
    xls = pd.ExcelFile(input_excel)
    
    # Loop through each sheet and convert to PDF
    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)
        
        # Create PDF with sheet name
        pdf_name = f"{sheet_name} - Readership.pdf"
        pdf = canvas.Canvas(pdf_name)
        
        # Write dataframe to PDF
        table_data = [sheet_df.columns] + sheet_df.values.tolist()
        for row_num, row_data in enumerate(table_data):
            for col_num, col_value in enumerate(row_data):
                pdf.drawString(col_num * 100, 800 - row_num * 15, str(col_value))
        
        # Save PDF
        pdf.save()

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)

import pandas as pd
from reportlab.pdfgen import canvas
from tabulate import tabulate

def excel_to_pdf(input_excel, output_pdf_folder):
    xls = pd.ExcelFile(input_excel)

    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)

        # Create PDF with sheet name
        pdf_name = f"{output_pdf_folder}{sheet_name} - Readership.pdf"
        pdf = canvas.Canvas(pdf_name)

        # Write dataframe to PDF using tabulate
        table_data = tabulate(sheet_df, headers='keys', tablefmt='grid')
        pdf.drawString(10, 800, table_data)

        # Save PDF
        pdf.save()

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Ensure the output folder exists
    import os
    os.makedirs(output_pdf_folder, exist_ok=True)

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)


import pandas as pd
from fpdf import FPDF

def excel_to_pdf(input_excel, output_pdf_folder):
    xls = pd.ExcelFile(input_excel)

    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)

        # Create PDF with sheet name
        pdf_name = f"{output_pdf_folder}{sheet_name} - Readership.pdf"
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_font("Arial", size=12)
        
        # Add sheet name as a header
        pdf.cell(200, 10, txt=f"{sheet_name} - Readership", ln=True, align='C')

        # Add dataframe as a table
        col_widths = [pdf.get_string_width(str(col)) for col in sheet_df.columns]
        max_width = max(col_widths)
        
        for i, col in enumerate(sheet_df.columns):
            pdf.cell(max_width + 6, 10, txt=str(col), border=1)
        
        pdf.ln()
        
        for _, row in sheet_df.iterrows():
            for col in sheet_df.columns:
                pdf.cell(max_width + 6, 10, txt=str(row[col]), border=1)
            pdf.ln()

        # Save PDF
        pdf.output(pdf_name)

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Ensure the output folder exists
    import os
    os.makedirs(output_pdf_folder, exist_ok=True)

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)

import pandas as pd
from fpdf import FPDF

def excel_to_pdf(input_excel, output_pdf_folder):
    xls = pd.ExcelFile(input_excel)

    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)

        # Create PDF with sheet name
        pdf_name = f"{output_pdf_folder}{sheet_name} - Readership.pdf"
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_font("Arial", size=12)
        
        # Add sheet name as a header
        pdf.cell(200, 10, txt=f"{sheet_name} - Readership", ln=True, align='C')

        # Add dataframe as a table
        col_widths = [pdf.get_string_width(str(col)) for col in sheet_df.columns]
        max_width = max(col_widths)
        
        for i, col in enumerate(sheet_df.columns):
            pdf.cell(max_width + 6, 10, txt=str(col), border=1)
        
        pdf.ln()
        
        for _, row in sheet_df.iterrows():
            for col in sheet_df.columns:
                pdf.cell(max_width + 6, 10, txt=str(row[col]), border=1)
            pdf.ln()

        # Save PDF with explicit encoding (utf-8)
        with open(pdf_name, "wb") as file:
            pdf.output(file)

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Ensure the output folder exists
    import os
    os.makedirs(output_pdf_folder, exist_ok=True)

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)


Sub ExportSheetsToPDF()
    Dim ws As Worksheet
    Dim savePath As String
    
    ' Set the folder path where PDFs will be saved
    savePath = "C:\Your\Output\Folder\"
    
    ' Loop through each sheet
    For Each ws In ThisWorkbook.Sheets
        ' Activate the sheet
        ws.Activate
        
        ' Construct the PDF file name based on the sheet name
        pdfFileName = savePath & ws.Name & " - Readership.pdf"
        
        ' Export the active sheet to PDF
        ActiveSheet.ExportAsFixedFormat Type:=xlTypePDF, Filename:=pdfFileName, Quality:=xlQualityStandard
    Next ws
End Sub



import re

def remove_special_characters(text):
    # Replace all special characters except commas and full stops
    cleaned_text = re.sub(r'[^a-zA-Z0-9,.\s]', '', text)
    return cleaned_text

# Example usage:
input_text = "This is a sample text with @special characters! Don't remove the comma, and the period."
result = remove_special_characters(input_text)
print(result)

import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming df is your DataFrame with columns Sentence, Name, and Index
# Sample DataFrame creation
data = {'Sentence': ['This is a sample sentence.', 'Another example sentence.'],
        'Name': ['John, Jane', 'Bob'],
        'Index': ['0, 4, 9, 13, 18, 22', '0, 3']}
df = pd.DataFrame(data)

# Load spaCy model
nlp = spacy.blank("en")

# Prepare training data
training_data = []
for _, row in df.iterrows():
    sentence = row['Sentence']
    names = row['Name'].split(', ')
    indices = [int(index) for index in row['Index'].split(', ')]
    
    entities = []
    for name in names:
        for start, end in zip(indices[::2], indices[1::2]):
            entities.append((start, end, 'PERSON'))  # Adjust the label as needed
    
    training_data.append((sentence, {'entities': entities}))

# Train the NER model
ner = nlp.get_pipe('ner')
for _, annotations in training_data:
    example = Example.from_dict(nlp.make_doc(_), annotations)
    ner.update([example])

# Save the trained model
nlp.to_disk("/path/to/your/ner_model")


import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming df is your DataFrame with columns Sentence, Name, and Index
# Sample DataFrame creation
data = {'Sentence': ['This is a sample sentence.', 'Another example sentence.'],
        'Name': ['John, Jane', 'Bob'],
        'Index': ['0, 4, 9, 13, 18, 22', '0, 3']}
df = pd.DataFrame(data)

# Load spaCy model
nlp = spacy.blank("en")

# Prepare training data
training_data = []
for _, row in df.iterrows():
    sentence = row['Sentence']
    names = row['Name'].split(', ')
    indices = [int(index) for index in row['Index'].split(', ')]
    
    entities = []
    for name in names:
        for start, end in zip(indices[::2], indices[1::2]):
            entities.append((start, end, 'PERSON'))  # Adjust the label as needed
    
    training_data.append((sentence, {'entities': entities}))

# Aggregate entities for each sentence
aggregated_training_data = {}
for sentence, entities in training_data:
    if sentence in aggregated_training_data:
        aggregated_training_data[sentence]['entities'].extend(entities['entities'])
    else:
        aggregated_training_data[sentence] = entities

# Convert aggregated data to list
final_training_data = [(sentence, entities) for sentence, entities in aggregated_training_data.items()]

# Train the NER model
ner = nlp.get_pipe('ner')
for sentence, annotations in final_training_data:
    example = Example.from_dict(nlp.make_doc(sentence), annotations)
    ner.update([example])

# Save the trained model
nlp.to_disk("/path/to/your/ner_model")


from fuzzywuzzy import process

def find_keywords(sentence, keyword_list, threshold=80):
    # Use fuzzywuzzy's process function to find best matches
    matches = process.extractBests(sentence, keyword_list, score_cutoff=threshold)

    # Extract matched keywords
    matched_keywords = [match[0] for match in matches]

    return matched_keywords

# Example sentence
example_sentence = "The quick brown fox jumped over the lazy dog."

# Example list of keywords
example_keywords = ["fox", "lazy", "cat", "quick"]

# Find keywords in the sentence
result = find_keywords(example_sentence, example_keywords)

# Print the result
print("Matched Keywords:", result)
