import spacy
from spacy.training.example import Example

# Load a blank English model
nlp = spacy.blank("en")

# Add a new entity label for custom person names
nlp.add_pipe("ner", config={"labels": ["CUSTOM_PERSON"]})

# Define your training data
# Example data format: (text, {"entities": [(start, end, "CUSTOM_PERSON")]})
training_data = [
    ("Jan did a great job but Sam needs improvement.", {"entities": [(0, 3, "CUSTOM_PERSON"), (28, 31, "CUSTOM_PERSON")]}),
    # Add more examples as needed
]

# Convert training data to spaCy format
examples = []
for text, annotations in training_data:
    example = Example.from_dict(nlp.make_doc(text), annotations)
    examples.append(example)

# Train the NER model
nlp.begin_training()
for epoch in range(10):  # You might need to adjust the number of epochs
    for example in examples:
        nlp.update([example], drop=0.5)  # You can adjust the dropout rate

# Save the trained model
nlp.to_disk("path/to/your/custom_ner_model")

import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming your DataFrame is named 'df' with columns 'Name', 'Sentence', and 'Index'
# Example data:
# df = pd.DataFrame({'Name': ['John', 'Alice', 'Bob', 'Charlie'],
#                    'Sentence': ['John is a programmer.', 'Alice likes coding.', 'Bob enjoys hiking.', 'Charlie and David are friends.'],
#                    'Index': [(0, 3), (0, 5), (0, 2), (0, 7)]})

# Load a blank English model
nlp = spacy.blank("en")

# Add a new entity label for custom person names
nlp.add_pipe("ner", config={"labels": ["CUSTOM_PERSON"]})

# Convert DataFrame to spaCy training data format
training_data = []

for _, row in df.iterrows():
    name = row['Name']
    sentence = row['Sentence']
    start, end = row['Index']

    # Create a training example in spaCy format
    example = Example.from_dict(nlp.make_doc(sentence), {"entities": [(start, end, "CUSTOM_PERSON")]})
    training_data.append(example)

# Train the NER model
nlp.begin_training()
for epoch in range(10):  # You might need to adjust the number of epochs
    for example in training_data:
        nlp.update([example], drop=0.5)  # You can adjust the dropout rate

# Save the trained model
nlp.to_disk("path/to/your/custom_ner_name_model")

import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming your DataFrame is named 'df' with columns 'Name', 'Sentence', and 'Index'
# Example data:
# df = pd.DataFrame({'Name': ['John', 'Alice', 'Bob', 'Charlie'],
#                    'Sentence': ['John is a programmer.', 'Alice likes coding.', 'Bob enjoys hiking.', 'Charlie and David are friends.'],
#                    'Index': [(0, 3), (0, 5), (0, 2), (0, 7)]})

# Load a blank English model
nlp = spacy.blank("en")

# Create a new entity label for custom person names
ner = nlp.add_pipe("ner")

# Convert DataFrame to spaCy training data format
training_data = []

for _, row in df.iterrows():
    name, sentence, index = row['Name'], row['Sentence'], row['Index']
    start, end = index

    # Create a training example in spaCy format
    example = Example.from_dict(nlp.make_doc(sentence), {"entities": [(start, end, "CUSTOM_PERSON")]})
    training_data.append(example)

# Define a callback to print training progress
def print_progress(info, epoch, loss, scorer):
    print(f"Epoch: {epoch}, Loss: {loss}, Progress: {info['progress']}")

# Train the NER model with the callback
nlp.begin_training(callback=print_progress)
for epoch in range(10):  # You might need to adjust the number of epochs
    for example in training_data:
        nlp.update([example], drop=0.5)  # You can adjust the dropout rate

# Save the trained model
nlp.to_disk("path/to/your/custom_ner_name_model")

import pandas as pd
from reportlab.pdfgen import canvas

def excel_to_pdf(input_excel, output_pdf):
    # Read all sheets from Excel file
    xls = pd.ExcelFile(input_excel)
    
    # Loop through each sheet and convert to PDF
    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)
        
        # Create PDF with sheet name
        pdf_name = f"{sheet_name} - Readership.pdf"
        pdf = canvas.Canvas(pdf_name)
        
        # Write dataframe to PDF
        table_data = [sheet_df.columns] + sheet_df.values.tolist()
        for row_num, row_data in enumerate(table_data):
            for col_num, col_value in enumerate(row_data):
                pdf.drawString(col_num * 100, 800 - row_num * 15, str(col_value))
        
        # Save PDF
        pdf.save()

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)

import pandas as pd
from reportlab.pdfgen import canvas
from tabulate import tabulate

def excel_to_pdf(input_excel, output_pdf_folder):
    xls = pd.ExcelFile(input_excel)

    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)

        # Create PDF with sheet name
        pdf_name = f"{output_pdf_folder}{sheet_name} - Readership.pdf"
        pdf = canvas.Canvas(pdf_name)

        # Write dataframe to PDF using tabulate
        table_data = tabulate(sheet_df, headers='keys', tablefmt='grid')
        pdf.drawString(10, 800, table_data)

        # Save PDF
        pdf.save()

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Ensure the output folder exists
    import os
    os.makedirs(output_pdf_folder, exist_ok=True)

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)


import pandas as pd
from fpdf import FPDF

def excel_to_pdf(input_excel, output_pdf_folder):
    xls = pd.ExcelFile(input_excel)

    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)

        # Create PDF with sheet name
        pdf_name = f"{output_pdf_folder}{sheet_name} - Readership.pdf"
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_font("Arial", size=12)
        
        # Add sheet name as a header
        pdf.cell(200, 10, txt=f"{sheet_name} - Readership", ln=True, align='C')

        # Add dataframe as a table
        col_widths = [pdf.get_string_width(str(col)) for col in sheet_df.columns]
        max_width = max(col_widths)
        
        for i, col in enumerate(sheet_df.columns):
            pdf.cell(max_width + 6, 10, txt=str(col), border=1)
        
        pdf.ln()
        
        for _, row in sheet_df.iterrows():
            for col in sheet_df.columns:
                pdf.cell(max_width + 6, 10, txt=str(row[col]), border=1)
            pdf.ln()

        # Save PDF
        pdf.output(pdf_name)

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Ensure the output folder exists
    import os
    os.makedirs(output_pdf_folder, exist_ok=True)

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)

import pandas as pd
from fpdf import FPDF

def excel_to_pdf(input_excel, output_pdf_folder):
    xls = pd.ExcelFile(input_excel)

    for sheet_name in xls.sheet_names:
        sheet_df = xls.parse(sheet_name)

        # Create PDF with sheet name
        pdf_name = f"{output_pdf_folder}{sheet_name} - Readership.pdf"
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_font("Arial", size=12)
        
        # Add sheet name as a header
        pdf.cell(200, 10, txt=f"{sheet_name} - Readership", ln=True, align='C')

        # Add dataframe as a table
        col_widths = [pdf.get_string_width(str(col)) for col in sheet_df.columns]
        max_width = max(col_widths)
        
        for i, col in enumerate(sheet_df.columns):
            pdf.cell(max_width + 6, 10, txt=str(col), border=1)
        
        pdf.ln()
        
        for _, row in sheet_df.iterrows():
            for col in sheet_df.columns:
                pdf.cell(max_width + 6, 10, txt=str(row[col]), border=1)
            pdf.ln()

        # Save PDF with explicit encoding (utf-8)
        with open(pdf_name, "wb") as file:
            pdf.output(file)

if __name__ == "__main__":
    input_excel_file = "your_input_excel.xlsx"
    output_pdf_folder = "output_pdfs/"

    # Ensure the output folder exists
    import os
    os.makedirs(output_pdf_folder, exist_ok=True)

    # Convert each sheet to PDF
    excel_to_pdf(input_excel_file, output_pdf_folder)


Sub ExportSheetsToPDF()
    Dim ws As Worksheet
    Dim savePath As String
    
    ' Set the folder path where PDFs will be saved
    savePath = "C:\Your\Output\Folder\"
    
    ' Loop through each sheet
    For Each ws In ThisWorkbook.Sheets
        ' Activate the sheet
        ws.Activate
        
        ' Construct the PDF file name based on the sheet name
        pdfFileName = savePath & ws.Name & " - Readership.pdf"
        
        ' Export the active sheet to PDF
        ActiveSheet.ExportAsFixedFormat Type:=xlTypePDF, Filename:=pdfFileName, Quality:=xlQualityStandard
    Next ws
End Sub



import re

def remove_special_characters(text):
    # Replace all special characters except commas and full stops
    cleaned_text = re.sub(r'[^a-zA-Z0-9,.\s]', '', text)
    return cleaned_text

# Example usage:
input_text = "This is a sample text with @special characters! Don't remove the comma, and the period."
result = remove_special_characters(input_text)
print(result)

import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming df is your DataFrame with columns Sentence, Name, and Index
# Sample DataFrame creation
data = {'Sentence': ['This is a sample sentence.', 'Another example sentence.'],
        'Name': ['John, Jane', 'Bob'],
        'Index': ['0, 4, 9, 13, 18, 22', '0, 3']}
df = pd.DataFrame(data)

# Load spaCy model
nlp = spacy.blank("en")

# Prepare training data
training_data = []
for _, row in df.iterrows():
    sentence = row['Sentence']
    names = row['Name'].split(', ')
    indices = [int(index) for index in row['Index'].split(', ')]
    
    entities = []
    for name in names:
        for start, end in zip(indices[::2], indices[1::2]):
            entities.append((start, end, 'PERSON'))  # Adjust the label as needed
    
    training_data.append((sentence, {'entities': entities}))

# Train the NER model
ner = nlp.get_pipe('ner')
for _, annotations in training_data:
    example = Example.from_dict(nlp.make_doc(_), annotations)
    ner.update([example])

# Save the trained model
nlp.to_disk("/path/to/your/ner_model")


import pandas as pd
import spacy
from spacy.training.example import Example

# Assuming df is your DataFrame with columns Sentence, Name, and Index
# Sample DataFrame creation
data = {'Sentence': ['This is a sample sentence.', 'Another example sentence.'],
        'Name': ['John, Jane', 'Bob'],
        'Index': ['0, 4, 9, 13, 18, 22', '0, 3']}
df = pd.DataFrame(data)

# Load spaCy model
nlp = spacy.blank("en")

# Prepare training data
training_data = []
for _, row in df.iterrows():
    sentence = row['Sentence']
    names = row['Name'].split(', ')
    indices = [int(index) for index in row['Index'].split(', ')]
    
    entities = []
    for name in names:
        for start, end in zip(indices[::2], indices[1::2]):
            entities.append((start, end, 'PERSON'))  # Adjust the label as needed
    
    training_data.append((sentence, {'entities': entities}))

# Aggregate entities for each sentence
aggregated_training_data = {}
for sentence, entities in training_data:
    if sentence in aggregated_training_data:
        aggregated_training_data[sentence]['entities'].extend(entities['entities'])
    else:
        aggregated_training_data[sentence] = entities

# Convert aggregated data to list
final_training_data = [(sentence, entities) for sentence, entities in aggregated_training_data.items()]

# Train the NER model
ner = nlp.get_pipe('ner')
for sentence, annotations in final_training_data:
    example = Example.from_dict(nlp.make_doc(sentence), annotations)
    ner.update([example])

# Save the trained model
nlp.to_disk("/path/to/your/ner_model")


from fuzzywuzzy import process

def find_keywords(sentence, keyword_list, threshold=80):
    # Use fuzzywuzzy's process function to find best matches
    matches = process.extractBests(sentence, keyword_list, score_cutoff=threshold)

    # Extract matched keywords
    matched_keywords = [match[0] for match in matches]

    return matched_keywords

# Example sentence
example_sentence = "The quick brown fox jumped over the lazy dog."

# Example list of keywords
example_keywords = ["fox", "lazy", "cat", "quick"]

# Find keywords in the sentence
result = find_keywords(example_sentence, example_keywords)

# Print the result
print("Matched Keywords:", result)

import re

# Sample data: Key-value pair of names and employee IDs
employee_id_mapping = {
    "John Doe": "JD123",
    "Jane Smith": "JS456",
    "Alice Johnson": "AJ789",
    # Add more entries as needed
}

# Example sentence
sentence = "John Doe and Jane Smith are working together."

# Regular expression to match full names
full_name_pattern = re.compile(r'\b([A-Z][a-z]+(?:\s[A-Z][a-z]+)*)\b')

# Extract full names from the sentence
full_names = re.findall(full_name_pattern, sentence)

# Look up employee IDs for each extracted name
employee_ids = [employee_id_mapping.get(name, "Not found") for name in full_names]

# Print the extracted names and corresponding employee IDs
for name, employee_id in zip(full_names, employee_ids):
    print(f"Name: {name}, Employee ID: {employee_id}")


import re

def extract_names_and_employee_ids(sentence, name_database):
    # Regular expression to match full names
    full_name_pattern = re.compile(r'\b([A-Z][a-z]+(?:\s[A-Z][a-z]+)*)\b')

    # Extract full names from the sentence
    full_names = re.findall(full_name_pattern, sentence)

    # Look up employee IDs for each extracted name
    name_employee_id_pairs = [(name, name_database.get(name, "Not found")) for name in full_names if name in name_database]

    return name_employee_id_pairs

# Sample data: Key-value pair of names and employee IDs
employee_id_mapping = {
    "John Doe": "JD123",
    "Jane Smith": "JS456",
    "Alice Johnson": "AJ789",
    # Add more entries as needed
}

# Example sentence
example_sentence = "John Doe and Jane Smith are working together."

# Call the function
result = extract_names_and_employee_ids(example_sentence, employee_id_mapping)

# Print the result
for name, employee_id in result:
    print(f"Name: {name}, Employee ID: {employee_id}")

import re

# Example sentence
sentence = "The service was great, very helpful. However, the response was slow."

# Define a regular expression pattern for the keywords
keyword_pattern = re.compile(r'\b(?:great|helpful|Not great|slow response)\b', re.IGNORECASE)

# Extract keywords from the sentence
keywords = re.findall(keyword_pattern, sentence)

# Print the extracted keywords
print("Extracted Keywords:", keywords)
import re

def extract_feedback(sentence, person_names):
    feedback_dict = {}

    for person_name in person_names:
        # Create a regular expression pattern for the feedback mentioning the person
        pattern = re.compile(fr'\b{re.escape(person_name)}\b:\s*([^\.,]+)', re.IGNORECASE)

        # Find the match in the sentence
        match = pattern.search(sentence)

        # If a match is found, extract the feedback
        if match:
            feedback = match.group(1).strip()
            feedback_dict[person_name] = feedback

    return feedback_dict

# Example sentence
example_sentence = "Great work Sameer but Jubel needs improvement."

# List of person names
person_names = ["Sameer", "Jubel"]

# Extract feedback for each person
result = extract_feedback(example_sentence, person_names)

# Print the result
for person_name, feedback in result.items():
    print(f"{person_name}: {feedback}")


import re

def calculate_feedback_types(feedbacks):
    # Your regex patterns
    name_pattern = r'(John|Alice|Bob|Charlie|Susan)'
    positive_pattern = r'(great|excellent|positive)'
    negative_pattern = r'(poor|needs improvement|negative)'

    # Function to calculate distance between two positions
    def calculate_distance(position1, position2):
        return abs(position1 - position2)

    # Function to segment sentences using pause-indicating words
    def segment_sentences(text):
        pause_words = ['fullstop', 'however', 'but', 'although']  # Add more as needed
        sentence_segments = re.split(r'\b(?:{})\b'.format('|'.join(pause_words)), text, flags=re.IGNORECASE)
        return [segment.strip() for segment in sentence_segments if segment.strip()]

    # Initialize data structure to store feedback types for each person
    feedback_types_by_person = {}

    # Iterate through feedbacks and extract information
    for feedback in feedbacks:
        # Segment sentences
        segments = segment_sentences(feedback)

        # Extract names and their positions for each segment
        for segment in segments:
            name_matches = re.finditer(name_pattern, segment, re.IGNORECASE)
            names_positions = [(match.group(), match.start()) for match in name_matches]

            # Extract positions of positive and negative keywords for each segment
            positive_matches = re.finditer(positive_pattern, segment, re.IGNORECASE)
            negative_matches = re.finditer(negative_pattern, segment, re.IGNORECASE)

            positive_positions = [match.start() for match in positive_matches]
            negative_positions = [match.start() for match in negative_matches]

            # Calculate distances and find closest keyword for each person in each segment
            for name, name_position in names_positions:
                distances_to_positive = [calculate_distance(name_position, pos) for pos in positive_positions]
                distances_to_negative = [calculate_distance(name_position, pos) for pos in negative_positions]

                closest_positive_index = distances_to_positive.index(min(distances_to_positive)) if distances_to_positive else None
                closest_negative_index = distances_to_negative.index(min(distances_to_negative)) if distances_to_negative else None

                closest_positive = "Positive" if closest_positive_index is not None else None
                closest_negative = "Negative" if closest_negative_index is not None else None

                # Store feedback type in the data structure for each person in each segment
                if name not in feedback_types_by_person:
                    feedback_types_by_person[name] = []

                if closest_positive is not None:
                    feedback_types_by_person[name].append(closest_positive)
                if closest_negative is not None:
                    feedback_types_by_person[name].append(closest_negative)

    return feedback_types_by_person

# Sample feedbacks
feedbacks = [
    "John and Alice did a great job! However, Bob's performance was poor. Needs improvement.",
    "Charlie received positive feedback for his excellent work, but Susan needs improvement."
]

# Call the function and print the results
result = calculate_feedback_types(feedbacks)
for person, feedback_types in result.items():
    print(f"{person}'s Feedback Types: {', '.join(feedback_types)}")

nevertheless
nonetheless
on the other hand
on the contrary
in contrast
even so
still
yet
meanwhile
afterward
subsequently
eventually
consequently
therefore
thus
moreover
furthermore
additionally
besides
similarly
likewise
in conclusion
to summarize
These words can be used to identify potential breaks or transitions in sentences 



import re
from textblob import TextBlob

def calculate_distance(position1, position2):
    return abs(position1 - position2)

def analyze_feedback_sentiment(feedback):
    analysis = TextBlob(feedback)
    return "Positive" if analysis.sentiment.polarity > 0 else "Negative" if analysis.sentiment.polarity < 0 else "Neutral"

def extract_feedback_information(feedback, name_database):
    name_pattern = r'\b(?:' + '|'.join(name_database.keys()) + r')\b'
    positive_pattern = r'\b(?:great|excellent|positive)\b'
    negative_pattern = r'\b(?:poor|needs improvement|negative)\b'
    pause_words = ['fullstop', 'however', 'but', 'although']  # Add more as needed

    name_matches = re.finditer(name_pattern, feedback, re.IGNORECASE)
    names_positions = [(match.group(), match.start()) for match in name_matches]

    positive_matches = re.finditer(positive_pattern, feedback, re.IGNORECASE)
    negative_matches = re.finditer(negative_pattern, feedback, re.IGNORECASE)

    positive_positions = [match.start() for match in positive_matches]
    negative_positions = [match.start() for match in negative_matches]

    for name, name_position in names_positions:
        distances_to_positive = [calculate_distance(name_position, pos) for pos in positive_positions]
        distances_to_negative = [calculate_distance(name_position, pos) for pos in negative_positions]

        closest_positive_index = distances_to_positive.index(min(distances_to_positive)) if distances_to_positive else None
        closest_negative_index = distances_to_negative.index(min(distances_to_negative)) if distances_to_negative else None

        closest_positive = "Positive" if closest_positive_index is not None else None
        closest_negative = "Negative" if closest_negative_index is not None else None

        if closest_positive is not None:
            feedback_keyword = "positive"
            sentiment = analyze_feedback_sentiment(feedback)
            return name, name_database.get(name, ""), feedback_keyword, feedback, sentiment
        elif closest_negative is not None:
            feedback_keyword = "negative"
            sentiment = analyze_feedback_sentiment(feedback)
            return name, name_database.get(name, ""), feedback_keyword, feedback, sentiment

    return None  # No matching feedback found

# Sample name database
name_database = {
    "John": "ID123",
    "Alice": "ID456",
    "Bob": "ID789"
}

# Sample feedbacks
feedbacks = [
    "John did a great job! However, Alice's performance needs improvement.",
    "Bob received positive feedback for his excellent work."
]

# Analyze feedback for each person
for feedback in feedbacks:
    result = extract_feedback_information(feedback, name_database)
    if result:
        name, employee_id, feedback_keyword, actual_feedback, sentiment = result
        print(f"Name: {name}, Employee ID: {employee_id}, Feedback Keyword: {feedback_keyword}, Feedback: {actual_feedback}, Sentiment: {sentiment}")




I apologize for the oversight. Let's enhance the code to break the sentence with pause words. We'll use a function to segment the sentence based on pause words and then analyze each segment for feedback information. Here's the updated code:

```python
import re
from textblob import TextBlob

def calculate_distance(position1, position2):
    return abs(position1 - position2)

def analyze_feedback_sentiment(feedback):
    analysis = TextBlob(feedback)
    return "Positive" if analysis.sentiment.polarity > 0 else "Negative" if analysis.sentiment.polarity < 0 else "Neutral"

def segment_sentence(text, pause_words):
    sentence_segments = re.split(r'\b(?:{})\b'.format('|'.join(pause_words)), text, flags=re.IGNORECASE)
    return [segment.strip() for segment in sentence_segments if segment.strip()]

def extract_feedback_information(segment, name_database):
    name_pattern = r'\b(?:' + '|'.join(name_database.keys()) + r')\b'
    positive_pattern = r'\b(?:great|excellent|positive)\b'
    negative_pattern = r'\b(?:poor|needs improvement|negative)\b'

    name_matches = re.finditer(name_pattern, segment, re.IGNORECASE)
    names_positions = [(match.group(), match.start()) for match in name_matches]

    positive_matches = re.finditer(positive_pattern, segment, re.IGNORECASE)
    negative_matches = re.finditer(negative_pattern, segment, re.IGNORECASE)

    positive_positions = [match.start() for match in positive_matches]
    negative_positions = [match.start() for match in negative_matches]

    for name, name_position in names_positions:
        distances_to_positive = [calculate_distance(name_position, pos) for pos in positive_positions]
        distances_to_negative = [calculate_distance(name_position, pos) for pos in negative_positions]

        closest_positive_index = distances_to_positive.index(min(distances_to_positive)) if distances_to_positive else None
        closest_negative_index = distances_to_negative.index(min(distances_to_negative)) if distances_to_negative else None

        closest_positive = "Positive" if closest_positive_index is not None else None
        closest_negative = "Negative" if closest_negative_index is not None else None

        if closest_positive is not None:
            feedback_keyword = "positive"
            sentiment = analyze_feedback_sentiment(segment)
            return name, name_database.get(name, ""), feedback_keyword, segment, sentiment
        elif closest_negative is not None:
            feedback_keyword = "negative"
            sentiment = analyze_feedback_sentiment(segment)
            return name, name_database.get(name, ""), feedback_keyword, segment, sentiment

    return None  # No matching feedback found

def analyze_feedback(feedback, name_database, pause_words):
    segments = segment_sentence(feedback, pause_words)

    for segment in segments:
        result = extract_feedback_information(segment, name_database)
        if result:
            return result  # Return the first matching result

    return None  # No matching feedback found

# Sample name database
name_database = {
    "John": "ID123",
    "Alice": "ID456",
    "Bob": "ID789"
}

# Sample feedbacks
feedbacks = [
    "John did a great job! However, Alice's performance needs improvement.",
    "Bob received positive feedback for his excellent work."
]

# Pause words
pause_words = ['fullstop', 'however', 'but', 'although']

# Analyze feedback for each person
for feedback in feedbacks:
    result = analyze_feedback(feedback, name_database, pause_words)
    if result:
        name, employee_id, feedback_keyword, actual_feedback, sentiment = result
        print(f"Name: {name}, Employee ID: {employee_id}, Feedback Keyword: {feedback_keyword}, Feedback: {actual_feedback}, Sentiment: {sentiment}")
```

Now the code uses the `segment_sentence` function to split the sentence based on pause words, and then it analyzes each segment for feedback information. Adjust the `pause_words` list as needed for your use case.


import re
from textblob import TextBlob

def calculate_distance(position1, position2):
    return abs(position1 - position2)

def analyze_feedback_sentiment(feedback):
    analysis = TextBlob(feedback)
    return "Positive" if analysis.sentiment.polarity > 0 else "Negative" if analysis.sentiment.polarity < 0 else "Neutral"

def segment_sentence(text, pause_words):
    sentence_segments = re.split(r'\b(?:{})\b'.format('|'.join(pause_words)), text, flags=re.IGNORECASE)
    return [segment.strip() for segment in sentence_segments if segment.strip()]

def extract_feedback_information(segment, name_database):
    name_pattern = r'\b(?:' + '|'.join(name_database.keys()) + r')\b'
    positive_pattern = r'\b(?:great|excellent|positive)\b'
    negative_pattern = r'\b(?:poor|needs improvement|negative)\b'

    name_matches = re.finditer(name_pattern, segment, re.IGNORECASE)
    names_positions = [(match.group(), match.start()) for match in name_matches]

    positive_matches = re.finditer(positive_pattern, segment, re.IGNORECASE)
    negative_matches = re.finditer(negative_pattern, segment, re.IGNORECASE)

    positive_positions = [match.start() for match in positive_matches]
    negative_positions = [match.start() for match in negative_matches]

    feedback_results = []

    for name, name_position in names_positions:
        distances_to_positive = [calculate_distance(name_position, pos) for pos in positive_positions]
        distances_to_negative = [calculate_distance(name_position, pos) for pos in negative_positions]

        closest_positive_index = distances_to_positive.index(min(distances_to_positive)) if distances_to_positive else None
        closest_negative_index = distances_to_negative.index(min(distances_to_negative)) if distances_to_negative else None

        closest_positive = "Positive" if closest_positive_index is not None else None
        closest_negative = "Negative" if closest_negative_index is not None else None

        if closest_positive is not None:
            feedback_keyword = "positive"
            sentiment = analyze_feedback_sentiment(segment)
            feedback_results.append((name, name_database.get(name, ""), feedback_keyword, segment, sentiment))
        elif closest_negative is not None:
            feedback_keyword = "negative"
            sentiment = analyze_feedback_sentiment(segment)
            feedback_results.append((name, name_database.get(name, ""), feedback_keyword, segment, sentiment))

    return feedback_results

def analyze_feedback(feedback, name_database, pause_words):
    segments = segment_sentence(feedback, pause_words)
    all_results = []

    for segment in segments:
        results = extract_feedback_information(segment, name_database)
        all_results.extend(results)

    return all_results

# Sample name database
name_database = {
    "John": "ID123",
    "Alice": "ID456",
    "Bob": "ID789"
}

# Sample feedbacks
feedbacks = [
    "John did a great job! However, Alice's performance needs improvement.",
    "Bob received positive feedback for his excellent work."
]

# Pause words
pause_words = ['fullstop', 'however', 'but', 'although']

# Analyze feedback for each segment
for feedback in feedbacks:
    results = analyze_feedback(feedback, name_database, pause_words)
    for result in results:
        name, employee_id, feedback_keyword, actual_feedback, sentiment = result
        print(f"Name: {name}, Employee ID: {employee_id}, Feedback Keyword: {feedback_keyword}, Feedback: {actual_feedback}, Sentiment: {sentiment}")

name_pattern = r'\b(?:' + '|'.join(re.escape(name) for name in name_database.keys()) + r')\b'


import re
from textblob import TextBlob

def calculate_distance(position1, position2):
    return abs(position1 - position2)

def analyze_feedback_sentiment(feedback):
    analysis = TextBlob(feedback)
    return "Positive" if analysis.sentiment.polarity > 0 else "Negative" if analysis.sentiment.polarity < 0 else "Neutral"

def segment_sentence(text, pause_words):
    # Escape the period in pause words
    escaped_pause_words = [re.escape(word) for word in pause_words]
    pattern = r'\b(?:{})\b'.format('|'.join(escaped_pause_words))
    
    sentence_segments = re.split(pattern, text, flags=re.IGNORECASE)
    return [segment.strip() for segment in sentence_segments if segment.strip()]

def extract_feedback_information(segment, name_database):
    # ... (unchanged)

def analyze_feedback(feedback, name_database, pause_words):
    segments = segment_sentence(feedback, pause_words)
    all_results = []

    for segment in segments:
        results = extract_feedback_information(segment, name_database)
        if results:
            all_results.extend(results)

    return all_results

# Sample name database, feedbacks, and pause words remain the same

# Analyze feedback for each segment
for feedback in feedbacks:
    results = analyze_feedback(feedback, name_database, pause_words)
    for result in results:
        name, employee_id, feedback_keyword, actual_feedback, sentiment = result
        print(f"Name: {name}, Employee ID: {employee_id}, Feedback Keyword: {feedback_keyword}, Feedback: {actual_feedback}, Sentiment: {sentiment}")

from punctuation_standardizer import standardize_punctuation

def correct_punctuations(sentence):
    corrected_sentence = standardize_punctuation(sentence)
    return corrected_sentence

# Example usage
original_sentence = "This is a sentence with some... misplaced punctuations!!"
corrected_sentence = correct_punctuations(original_sentence)

print(f"Original Sentence: {original_sentence}")
print(f"Corrected Sentence: {corrected_sentence}")
