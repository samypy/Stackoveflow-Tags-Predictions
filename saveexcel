# Input data with other columns and a column containing values like "abcd:40,efgh:50"
data = [
    {"name": "John", "age": 30, "column_name": "abcd:40,efgh:50"},
    {"name": "Alice", "age": 25, "column_name": "ijkl:60,mnop:70"},
    # Add more data as needed
]

# Create a list to store the resulting rows
result_rows = []

# Iterate over the input data
for row in data:
    other_columns = {key: value for key, value in row.items() if key != "column_name"}
    column_value = row["column_name"]

    # Split the column value using ',' as the delimiter
    items = column_value.split(',')

    # Iterate over the split items
    for item in items:
        # Split each item using ':' as the delimiter
        name, duration = item.split(':')

        # Create a new row with other columns and the split 'name' and 'duration'
        new_row = other_columns.copy()
        new_row["Name"] = name
        new_row["Duration"] = int(duration)

        # Append the new row to the result
        result_rows.append(new_row)

# Print the result
for row in result_rows:
    print(row)

from transformers import BertTokenizer, BertForSequenceClassification
import torch
import re

def break_sentences(sentence):
    # Define keywords to break sentences
    break_keywords = ['.', 'but', 'however', 'though', 'although', 'yet', 'so']

    # Create a regular expression pattern for splitting
    pattern = '|'.join(re.escape(keyword) for keyword in break_keywords)

    # Split the sentence using the defined pattern
    segments = re.split(pattern, sentence)

    # Remove empty segments and strip whitespace
    segments = [segment.strip() for segment in segments if segment.strip()]

    return segments

def load_local_bert_model(model_path):
    # Load locally stored BERT model and tokenizer
    model = BertForSequenceClassification.from_pretrained(model_path)
    tokenizer = BertTokenizer.from_pretrained(model_path)

    return model, tokenizer

def identify_feedback(sentences, model, tokenizer):
    # Prepare sentences for BERT input
    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

    # Make predictions using the BERT model
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract sentiment labels from the model output
    sentiments = torch.argmax(outputs.logits, dim=1).tolist()

    feedback_results = list(zip(sentences, sentiments))

    return feedback_results

# Example sentence
example_sentence = "I like the product but service is bad. However, the delivery was fast."

# Path to the locally stored BERT model
local_model_path = 'path/to/your/local/model'

# Load the locally stored BERT model and tokenizer
bert_model, bert_tokenizer = load_local_bert_model(local_model_path)

# Break the sentence into segments
broken_sentences = break_sentences(example_sentence)

# Identify feedback for each segment using the local BERT model
feedback_results = identify_feedback(broken_sentences, bert_model, bert_tokenizer)

print("Feedback for Each Segment:")
for result in feedback_results:
    segment, sentiment_label = result
    print(f"Segment: {segment}, Sentiment: {sentiment_label}")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
import string

# Download NLTK resources (if not downloaded)
nltk.download('punkt')
nltk.download('stopwords')

# Sample labeled dataset (replace with your own dataset)
sentences = ["I love this product!", "The service was terrible.", "Great experience overall.", "Not satisfied with the quality."]
labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative

# Text preprocessing
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())
    
    # Removing punctuation and stopwords
    tokens = [stemmer.stem(token) for token in tokens if token.isalnum() and token not in stop_words]
    
    return " ".join(tokens)

# Apply text preprocessing to each sentence
preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]

# Convert text to bag-of-words representation
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(preprocessed_sentences).toarray()

# Convert to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(labels, dtype=torch.float32).view(-1, 1)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Define the neural network model
class SentimentModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SentimentModel, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        _, (x, _) = self.lstm(x)
        x = x.squeeze(0)
        x = self.fc(x)
        x = self.sigmoid(x)
        return x

# Model parameters
input_size = X_train.shape[1]
hidden_size = 64
output_size = 1

# Instantiate the model
model = SentimentModel(input_size, hidden_size, output_size)

# Loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluation on the test set
model.eval()
with torch.no_grad():
    test_outputs = model(X_test)
    predicted_labels = (test_outputs > 0.5).float()

accuracy = accuracy_score(y_test.numpy(), predicted_labels.numpy())
print(f'Accuracy on the test set: {accuracy:.4f}')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
import string

# Download NLTK resources (if not downloaded)
nltk.download('punkt')
nltk.download('stopwords')

# Sample labeled dataset (replace with your own dataset)
sentences = ["I love this product!", "The service was terrible.", "Great experience overall.", "Not satisfied with the quality."]
labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative

# Text preprocessing
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())
    
    # Removing punctuation and stopwords
    tokens = [stemmer.stem(token) for token in tokens if token.isalnum() and token not in stop_words]
    
    return " ".join(tokens)

# Custom dataset class
class SentimentDataset(Dataset):
    def __init__(self, sentences, labels):
        self.sentences = sentences
        self.labels = labels

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return self.sentences[idx], self.labels[idx]

# Apply text preprocessing to each sentence
preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]

# Convert text to bag-of-words representation
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(preprocessed_sentences).toarray()

# Convert to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.long)  # Use long for indices
y_tensor = torch.tensor(labels, dtype=torch.float32).view(-1, 1)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Create custom datasets and dataloaders
train_dataset = SentimentDataset(X_train, y_train)
test_dataset = SentimentDataset(X_test, y_test)

train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=2)

# Define the neural network model
class SentimentModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SentimentModel, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        _, (x, _) = self.lstm(x)
        x = x.squeeze(0)
        x = self.fc(x)
        x = self.sigmoid(x)
        return x

# Model parameters
input_size = X_train.shape[1]
hidden_size = 64
output_size = 1

# Instantiate the model
model = SentimentModel(input_size, hidden_size, output_size)

# Loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    for inputs, labels in train_dataloader:
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # Backward pass and optimization
        loss.backward()
        optimizer.step()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluation on the test set
model.eval()
with torch.no_grad():
    all_preds = []
    all_labels = []
    for inputs, labels in test_dataloader:
        outputs = model(inputs)
        predicted_labels = (outputs > 0.5).float()
        all_preds.extend(predicted_labels.numpy())
        all_labels.extend(labels.numpy())

accuracy = accuracy_score(all_labels, all_preds)
print(f'Accuracy on the test set: {accuracy:.4f}')



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Sample data (replace this with your own dataset)
data = {'text': ['I love this product!', 'Not bad, but could be better.', 'Terrible experience.',
                 'Excellent service!', 'Average performance.']}
labels = ['positive', 'neutral', 'negative', 'positive', 'neutral']

df = pd.DataFrame({'text': data['text'], 'label': labels})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# Text preprocessing and feature extraction using CountVectorizer and TfidfTransformer
count_vectorizer = CountVectorizer()
tfidf_transformer = TfidfTransformer()

X_train_counts = count_vectorizer.fit_transform(X_train)
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

X_test_counts = count_vectorizer.transform(X_test)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

# Define a list of classifiers
classifiers = [
    ('Multinomial Naive Bayes', MultinomialNB()),
    ('Logistic Regression', LogisticRegression()),
    ('Random Forest', RandomForestClassifier()),
    ('Support Vector Machine', SVC())
]

# Train and evaluate each classifier
for name, classifier in classifiers:
    classifier.fit(X_train_tfidf, y_train)
    y_pred = classifier.predict(X_test_tfidf)

    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    print(f'\n{name}:\nAccuracy: {accuracy}\nClassification Report:\n{report}')

